{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.misc import derivative\n",
    "import scipy.integrate as intg\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow.keras\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.initializers import Constant, RandomNormal\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from astroNN.nn.layers import MCDropout\n",
    "\n",
    "sys.path.append(\"/home/isidro/Documents/github/nnogada/\")\n",
    "from nnogada import Nnogada\n",
    "\n",
    "np.random.seed(0)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# absolute path to search all text files inside a specific folder\n",
    "path = r'/home/isidro/Documents/github/model_independent_RC/data/MassModels/*.NFW.fix.REV.dat'\n",
    "files = glob.glob(path)\n",
    "print(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in files:\n",
    "    print(file)\n",
    "\n",
    "len(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_regression_dropout(num_nodes, num_hidden_layers):\n",
    "    # Defeine Keras model for regression\n",
    "    model = tf.keras.models.Sequential()\n",
    "    model.add(tf.keras.layers.InputLayer(batch_input_shape=((None, 1))))\n",
    "    for _ in range(num_hidden_layers):\n",
    "        model.add(Dense(units=num_nodes, activation='relu'))\n",
    "        model.add(MCDropout(0.2))\n",
    "    model.add(Dense(units=2, activation=\"linear\"))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_all = []\n",
    "\n",
    "for idx, file in enumerate(files):\n",
    "    print(\"Model {}/{}\".format(idx+1, len(files)))\n",
    "    print(\"-\"*10)\n",
    "    \n",
    "    history_ind = {}\n",
    "    \n",
    "    data = np.loadtxt(files[idx], skiprows=12) \n",
    "    df = pd.DataFrame(data, columns=['Radius', 'vgas', 'vdisk', 'vbulge', 'vobs', 'err_vobs', \n",
    "                                     'Vu', 'Vt','Rxv', 'Vxy'])\n",
    "    N = len(df.values)\n",
    "    print(\"Len: \", N)\n",
    "    print(df.head())\n",
    "    randomize = np.random.permutation(N)\n",
    "    data = df.values[randomize]\n",
    "\n",
    "    x = data[:,0]\n",
    "    y = data[:,1:3]\n",
    "    \n",
    "    scalerx = StandardScaler()\n",
    "    scalerx.fit(x.reshape(-1,1))\n",
    "    # apply transform\n",
    "    x = scalerx.transform(x.reshape(-1,1))    \n",
    "    split = 0.8\n",
    "    ntrain = int(split * len(x))\n",
    "    indx = [ntrain]\n",
    "    x_train, x_test = np.split(x, indx)\n",
    "    y_train, y_test = np.split(y, indx)\n",
    "    print(\"X_train shape: {} | y_train shape: {} | x_test shape: {} | y_test shape: {}\".format(np.shape(x_train), \n",
    "                                                                                               np.shape(y_train), \n",
    "                                                                                               np.shape(x_test), \n",
    "                                                                                               np.shape(y_test)))\n",
    "\n",
    "    population_size = 4  # max of individuals per generation\n",
    "    max_generations = 10    # number of generations\n",
    "    gene_length = 8        # lenght of the gene, depends on how many hiperparameters are tested 2*hyp\n",
    "    k = 1                  # num. of finalist individuals\n",
    "\n",
    "    # Define the hyperparameters for the search\n",
    "    hyperparams = {'deep': [3, 4], 'num_units': [100, 200], 'batch_size': [8, 16]}\n",
    "\n",
    "    # generate a Nnogada instance\n",
    "    net_fit = Nnogada(hyp_to_find=hyperparams, X_train=x_train, Y_train=y_train, X_val=x_test, Y_val=y_test, \n",
    "                      neural_library='keras', regression=True)\n",
    "    # Set the possible values of hyperparameters and not use the default values from hyperparameters.py\n",
    "    net_fit.set_hyperparameters()\n",
    "          \n",
    "    best_population = net_fit.ga_with_elitism(population_size, max_generations, gene_length, k)\n",
    "          \n",
    "    deep = int(best_population['deep'])\n",
    "    num_units = int(best_population['num_units'])\n",
    "    batch_size = int(best_population['batch_size'])\n",
    "            \n",
    "    # optimizer = Adam(lr=.005)\n",
    "    optimizer = Adam(lr=0.0005)\n",
    "\n",
    "    # Compile Keras model\n",
    "    model = model_regression_dropout(num_nodes=num_units, num_hidden_layers=deep)\n",
    "#     print(model.summary())\n",
    "          \n",
    "    model.compile(loss='mse', optimizer=optimizer) \n",
    "  \n",
    "    model_train = model.fit(x_train, y_train, batch_size=batch_size,\n",
    "                            epochs=1000, verbose=0,\n",
    "                            validation_data=(x_test, y_test))\n",
    "    \n",
    "#     model_train.history['val_loss'][-1]\n",
    "                  \n",
    "    history_ind['idx'] = idx\n",
    "    history_ind['deep'] = deep\n",
    "    history_ind['batch_size'] = batch_size\n",
    "    history_ind['num_units'] = num_units\n",
    "    history_ind['loss'] = model_train.history['val_loss'][-1]\n",
    "          \n",
    "    history_all.append(history_ind)   \n",
    "         \n",
    "    model.save('models/RC_model_{}.h5'.format(idx))\n",
    "  \n",
    "    # Generate test data\n",
    "    test_batch_size = 1000\n",
    "    # x_test = np.random.uniform(0, 2., test_batch_size)\n",
    "    x_test_to_pred = np.linspace(min(df['Radius'].values)-0.1, max(df['Radius'].values)+0.1, test_batch_size)\n",
    "\n",
    "    mc_dropout_num = 100  # Run Dropout 100 times\n",
    "    predictions = np.zeros((mc_dropout_num, test_batch_size, 2))\n",
    "    uncertainty = np.zeros((mc_dropout_num, test_batch_size, 1))\n",
    "    for i in range(mc_dropout_num):\n",
    "        predictions[i] = model.predict(scalerx.transform(x_test_to_pred.reshape(-1,1)))\n",
    "\n",
    "    # get mean results and its varience\n",
    "    prediction_mc_dropout = np.mean(predictions, axis=0)\n",
    "    std_mc_dropout = np.std(predictions, axis=0)\n",
    "    \n",
    "    # plt.rcParams[\"figure.figsize\"] = [7.50, 3.50]\n",
    "    plt.rcParams[\"figure.autolayout\"] = True\n",
    "    overlapping = 0.6\n",
    "\n",
    "    # Plotting\n",
    "    plt.figure(figsize=(10, 7), dpi=100)\n",
    "    sigma = np.sqrt(std_mc_dropout[:, 0]**2 + std_mc_dropout[:, 1]**2+ prediction_mc_dropout[:,1]**2)\n",
    "\n",
    "    plt.plot(df['Radius'].values, df['Vt'].values, color='k', linewidth=3, label='NFW theory',alpha=1)\n",
    "\n",
    "    plt.errorbar(df['Radius'].values, df['vobs'].values, yerr=df['err_vobs'].values, fmt='.', \n",
    "                 color='red', elinewidth=1, ecolor='red', markersize=5, label='Observations', alpha=overlapping)\n",
    "\n",
    "    plt.errorbar(x_test_to_pred, prediction_mc_dropout[:,0], yerr=sigma, markersize=2, fmt='o', \n",
    "                 ecolor='green', capthick=2, elinewidth=0.5, alpha=overlapping-0.2, c='green',\n",
    "                 label='Neural reconstruction')\n",
    "\n",
    "\n",
    "    plt.ylabel(\"$V(r)$\", fontsize=20)\n",
    "    plt.xlabel(\"r\", fontsize=20)\n",
    "    plt.xticks(fontsize=15)\n",
    "    plt.yticks(fontsize=15)\n",
    "\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.savefig(\"neural_reconstruction_RC_{}.png\".format(idx), dpi=100)\n",
    "          \n",
    "    plt.figure(figsize=(8, 6), dpi=100)\n",
    "\n",
    "    plt.plot(model_train.history['loss'], color='r', )\n",
    "    plt.plot(model_train.history['val_loss'], color='g')\n",
    "\n",
    "    plt.ylabel('MSE', fontsize=11)\n",
    "    plt.xlabel('Epoch', fontsize=11)\n",
    "    plt.legend(['training set', 'validation set'], loc='upper right', fontsize=12)\n",
    "    plt.xticks(fontsize=10)\n",
    "    plt.savefig('loss_fn_neural_RC_{}'.format(idx), dpi=100)\n",
    "    print(\"-\"*10)\n",
    "    # plt.ylim(0, 200)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
